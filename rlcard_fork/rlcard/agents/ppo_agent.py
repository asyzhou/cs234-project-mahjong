'''
PPO Agent -- uses code from hw assign2, originally modified from dqn agent
'''
import os
import random
import numpy as np
import torch
import torch.nn as nn
from collections import namedtuple
from copy import deepcopy
from policy_gradient import PolicyGradient
from rlcard.utils.utils import remove_illegal

Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done', 'legal_actions'])


class PPOAgent(PolicyGradient):
    '''
    PPO Agent, similar to assign2 but includes methods to test the agent in game
    '''
    def __init__(self):
        '''
        instantiate with policy gradient stuff
        '''
        # TODO: fill this in #
        return

    def step(self, state):
        ''' Predict the action for generating training data but
            have the predictions disconnected from the computation graph

        Args:
            state (numpy.array): current state

        Returns:
            action (int): an action id
        '''
        # TODO: fill this in #
        return 

    def eval_step(self, state):
        ''' Predict the action for evaluation purpose.

        Args:
            state (numpy.array): current state

        Returns:
            action (int): an action id
            info (dict): A dictionary containing information
        '''
        # TODO: fill this in #
        return 

    def update_policy(self, observations, actions, advantages, old_logprobs):
        '''
        Perform one update on the policy using the provided data using the PPO clipped
        objective function.
        '''
        # TODO: fill this in #

        return

    def train(self):
        ''' 
        Train the network
        '''
        # TODO: fill this in #
        return
    
    def sample_path(self, env, num_episodes=None):
        '''
        Sample paths from the environment for training; we might not need this
        '''
        return

    def set_device(self, device):
        # TODO: check what needs to be set to device #

        self.device = device
        # self.q_estimator.device = device
        # self.target_estimator.device = device

    def checkpoint_attributes(self):
        '''
        Return the current checkpoint attributes (dict)
        Checkpoint attributes are used to save and restore the model in the middle of training
        Saves the model state dict, optimizer state dict, and all other instance variables
        '''
        # TODO: check if we want this and implement #
        
    @classmethod
    def from_checkpoint(cls, checkpoint):
        '''
        Restore the model from a checkpoint
        
        Args:
            checkpoint (dict): the checkpoint attributes generated by checkpoint_attributes()
        '''
        # TODO: check if we want this and implement #
                     
    def save_checkpoint(self, path, filename='checkpoint_dqn.pt'):
        ''' Save the model checkpoint (all attributes)

        Args:
            path (str): the path to save the model
            filename(str): the file name of checkpoint
        '''
        torch.save(self.checkpoint_attributes(), os.path.join(path, filename))

